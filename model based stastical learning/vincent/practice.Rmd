---
title: "EM algorithm for handling missing data"
author: "Aude Sportisse & Vincent Vandewalle"
date: "03/11/2024"
output: html_document
---

```{r}
library(mvtnorm) #library for multivariate normal density
library(ggplot2) #library to have nice graphics
```


### Code an EM algorithm

We consider $X\sim \mathcal{N}(\mu,\Sigma)$, with
    $$\mu=\begin{pmatrix} 5 \\ -1
    \end{pmatrix} \textrm{ and } \Sigma=\begin{pmatrix} 1 & 0.5 \\ 0.5 & 1
    \end{pmatrix}.$$ We want to introduce $r=30\%$ of missing values in the variable $X_2$. We consider that the missing-data mechanism is MCAR.
  

**Q1)** Generate a bivariate normal set of sample size $n=100$, with mean $\mu$ and covariance matrix $\Sigma$ (use the package `mvtnorm`).

```{r}
# Define parameters

n <- 100 # Sample size
mu <- c(5, -1) # Mean vector
sigma <- matrix(c(10, 0.5, 0.5, 1), nrow = 2) # Covariance matrix

```

```{r}
mu
sigma

```

```{r}
data <- rmvnorm(n, mean = mu, sigma = sigma)

data_df <- data.frame(X1 = data[, 1], X2 = data[, 2])

head(data_df)

```
```{r}
#data
```


**Q2)** Introduce MCAR missing values in $X_2$. 

```{r}
# P(Mi2=1/ Xi1, Xi2)=0.3
idx = rbinom(n, 1, 0.3)  #(no of samples, number of trails, probability of success)
idx
```

```{r}

```

```{r}
data_df_obs = data_df
data_df_obs [ idx == 1, 2] = NA
```

```{r}
#data_df_obs = data_df
#data_df
#data_df_obs [ idx == 1,]
```


```{r}
data_df_obs
```

```{r}
data_df_obs[,2]
```



```{r}

mu1 = mean( data_df_obs[,1])
mu2 = mean( data_df_obs[,2], na.rm = TRUE)
```


```{r}
mu1
mu2
```

```{r}
cov_matrix <- cov(data_df_obs, use = "complete.obs")

```


```{r}
cov_matrix
```


The goal is now to estimate the parameters $\mu$ and $\Sigma$ in presence of missing values in $X_2$ by using the EM algorithm.

**Q3)** Propose a simple initialization for the EM algorithm. 

```{r}
mu1
mu2
cov_matrix
```


**Q4)** Write a function for the E-step and the M-step. 

```{r}
data_df_obs[,1]
```


```{r}
s1 = sum(data_df_obs[,1])
s11 = sum((data_df_obs[,1])^2)

```


```{r}
data_df_miss = is.na(data_df_obs[,2])
data_df_miss
```

```{r}
  X1 <- data_df_obs$X1
  X2 <- data_df_obs$X2
  
  obs_indices <- !is.na(X2)
  miss_indices <- is.na(X2)
  
```


```{r}
obs_indices

miss_indices
```


```{r}

  sigma11 <- cov_matrix[1, 1]
  sigma12 <- cov_matrix[1, 2]
  sigma22 <- cov_matrix[2, 2]
```


```{r}
X2[obs_indices]
```


```{r}
data_df_obs[obs_indices,2]
```



```{r}
e_step <- function(data_df_obs, mu1, mu2, cov_matrix){
  
  
    X1 <- data_df_obs$X1
  X2 <- data_df_obs$X2
  n=nrow(data_df_obs)
  obs_indices <- !is.na(X2)
  miss_indices <- is.na(X2)
  
    sigma11 <- cov_matrix[1, 1]
  sigma12 <- cov_matrix[1, 2]
  sigma22 <- cov_matrix[2, 2]
  

r21 <- sigma12 / sigma11
r22 <- sigma22 - (r21^2 * sigma11)


s2_obs <- sum(X2[obs_indices]) 
s22_obs <- sum(X2[obs_indices]^2) 
s12_obs <- sum(X1[obs_indices] * X2[obs_indices]) 


exp_S2 <- mu2 + r21 * (X1[miss_indices] - mu1)
exp_S22 <- (mu2 + r21 * (X1[miss_indices] - mu1))^2 + r22
exp_S12 <- X1[miss_indices] * (mu2 + r21 * (X1[miss_indices] - mu1))

S1 = sum(data_df_obs[,1])
S11 = sum((data_df_obs[,1])^2)

S2 <- s2_obs+ exp_S2
S22 <- s22_obs + exp_S22
S12 <- s12_obs + exp_S12

return(list(s1 = S1, s11 = S11, s2 = S2, s22 = S22, s12 = S12))
}

```

```{r}
sufficient_stats <- e_step(data_df_obs,mu1, mu2, cov_matrix)
```


```{r}
sufficient_stats
```


```{r}

m_step <- function(data_df_obs,sufficient_stats) {
  
  n=nrow(data_df_obs)
  # Extract sufficient statistics
  s1 <- sufficient_stats$s1
  s11 <- sufficient_stats$s11
  s2 <- sufficient_stats$s2
  s22 <- sufficient_stats$s22
  s12 <- sufficient_stats$s12
  
  # Update mean
  mu1 <- s1 / n
  mu2 <- s2 / n
  
  # Update covariance matrix
  sigma11 <- s11 / n - mu1^2
  sigma22 <- s22 / n - mu2^2
  sigma12 <- s12 / n - mu1 * mu2
  
  mu <- c(mu1, mu2)
  sigma <- matrix(c(sigma11, sigma12, sigma12, sigma22), nrow = 2)
  
  return(list(mu = mu, sigma = sigma))
}

```


```{r}
for (i in 1:50){
  E= e_step(data_df_obs, mu1, mu2, cov_matrix)
  print(E)
  M= m_step(data_df_obs, E)
}

```


**Q5)** Use the EM algorithm for $50$ iterations to estimate $\mu$ and $\Sigma$. Show the results. 

```{r}

```

```{r}

```


### Additionnal questions

**Q6)** Vary $n$ and the percentage of missing values.

**Q7)** We have estimated the parameters $\mu$ and $\Sigma$, can we impute the missing values? Try it!

**Q8)** Do you think the algorithm will still work for MNAR data? If you have the time, try it!

**Q9)** How to stop the EM algorithm? (other than by giving a predefined number of steps) 

**Q10)** Extend the code for considering missing values in both $X_1$ and $X_2$.

### Extra questions

**Q11)** Implement the EM algorithm for a mixture of two bivariate normal distributions.

**Q12)** Extend the algorithm for a multivariate normal distribution with $p$ variables with a missing-data mechanism MAR.


**Q13)** Extend the algorithm for a multivariate normal distribution with $p$ variables with a missing-data mechanism MAR and with sparse covariance matrix structure. 

**Q14)** Implement the EM algorithm for a mixture of two multivariate normal distributions with $p$ variables.

**Q15**) Implement the EM algorithm for a mixture of two multivariate normal distributions with $p$ variables and with sparse covariance matrix structure.

