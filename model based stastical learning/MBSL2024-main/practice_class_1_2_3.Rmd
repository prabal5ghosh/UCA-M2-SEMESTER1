---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


To illustrate the LDA on Iris data set:

```{r}
library(MASS)
?lda
```






```{r}
??knn
```




```{r}
matrix(3,5,6)
```


```{r}
# Load necessary libraries
library(caret)
library(class)

# Load the iris dataset
data(iris)

# Set up cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train the KNN model
knn_model <- train(Species ~ ., data = iris, method = "knn", trControl = train_control)

# Print the results
print(knn_model)

```




```{r}
V = 15
fold = rep(1:V,nrow(X)/V)
err.lda = err.knn = rep(NA,V)
for (v in 1:V){
  # Split the data
  X.learn = X[fold!=v,]
  Y.learn = Y[fold!=v]
  X.val = X[fold==v,]
  Y.val = Y[fold==v]
  
  # Learn the classifier
  f.lda = lda(X.learn,Y.learn)
  
  # Predict and evaluate the perf
  out.lda = predict(f.lda,X.val)
  out.knn = knn(X.learn,X.val,Y.learn,k=3)
  err.lda[v] = sum(out.lda$class != Y.val) / length(Y.val)
  err.knn[v] = sum(out.knn != Y.val) / length(Y.val)
}

cat("Err LDA:",mean(err.lda),"+/-",sd(err.lda),"\n")
cat("Err 3-NN:",mean(err.knn),"+/-",sd(err.knn),"\n")
```



```{r}
X = iris[, -5]
Y = iris$Species
```



```{r}
V = 15
fold = rep(1:V,nrow(X)/V)
err.lda = err.knn = rep(NA,V)
for (v in 1:V){
  # Split the data
  X.learn = X[fold!=v,]
  Y.learn = Y[fold!=v]
  X.val = X[fold==v,]
  Y.val = Y[fold==v]
  
  # Learn the classifier
  f.lda = lda(X.learn,Y.learn)
  
  # Predict and evaluate the perf
  out.lda = predict(f.lda,X.val)
  out.knn = knn(X.learn,X.val,Y.learn,k=3)
  err.lda[v] = sum(out.lda$class != Y.val) / length(Y.val)
  err.knn[v] = sum(out.knn != Y.val) / length(Y.val)
}

cat("Err LDA:",mean(err.lda),"+/-",sd(err.lda),"\n")
cat("Err 3-NN:",mean(err.knn),"+/-",sd(err.knn),"\n")
```
```{r}
library(class)

```





```{r}
knn.bestk <- function(X, Y, V=15, K.max=30){
  fold = rep(1:V, nrow(X)/V)
  err.knn = matrix(NA, K.max, V)
  for (k in 1:K.max){
    for (v in 1:V){
    # Split the data
    X.learn = X[fold!=v,]
    Y.learn = Y[fold!=v]
    X.val = X[fold==v,]
    Y.val = Y[fold==v]
    # Predict and evaluate the perf
    out.knn = knn(X.learn,X.val,Y.learn,k)
    err.knn[k, v] = sum(out.knn != Y.val) / length(Y.val)
    }
  }
return(which.min(rowMeans(err.knn)))
}
knn.bestk(X, Y)
```


```{r}
V = 15
fold = rep(1:V,nrow(X)/V)
err.lda = err.knn = rep(NA,V)
k.star = rep(NA,V)
for (v in 1:V){
  # Split the data
  X.learn = X[fold!=v,]
  Y.learn = Y[fold!=v]
  X.val = X[fold==v,]
  Y.val = Y[fold==v]
  
  # Learn and predict for LDA
  f.lda = lda(X.learn,Y.learn)
  out.lda = predict(f.lda,X.val)
  
  # Find the best k for k-nn and predict
  k.star[v] = knn.bestk(X.learn,Y.learn)
  out.knn = knn(X.learn,X.val,Y.learn,k=k.star[v])
  
  # Evaluate the errors
  err.lda[v] = sum(out.lda$class != Y.val) / length(Y.val)
  err.knn[v] = sum(out.knn != Y.val) / length(Y.val)
}

cat("Err LDA:",mean(err.lda),"+/-",sd(err.lda),"\n")
cat("Err k-NN:",mean(err.knn),"+/-",sd(err.knn),"\n")
cat("Average k for k-NN:",median(k.star))
```













```{r}
pairs(X, col=Y)
```



```{r}
Y.bin= as.numeric(Y)
Y.bin[Y.bin==3]==2

data = data.frame(X=X, Y= as.factor(Y))
f.lreg = glm(Y~., data= data, family= "binomial")
```

```{r}
f.lreg
```



```{r}
data[151,] = c(6,2.8,5,1.6, NA)
```



```{r}
out.lreg = predict(f.lreg, data[151, 1:4], type = "response")
out.lreg
```





```{r}

```












```{r}
Y.bin = as.numeric(Y)
Y.bin[Y.bin==2] = 1
Y.bin[Y.bin==3] = 2
```

```{r}
V = 15
fold = rep(1:V,nrow(X)/V)
err.lda = err.knn = err.lreg = rep(NA,V)
k.star = rep(NA,V)
for (v in 1:V){
  # Split the data
  X.learn = X[fold!=v,]
  Y.learn = Y.bin[fold!=v]
  X.val = X[fold==v,]
  Y.val = Y.bin[fold==v]
  
  # Learn and predict for LDA
  f.lda = lda(X.learn,Y.learn)
  out.lda = predict(f.lda,X.val)
  
  # Find the best k for k-nn and predict
  k.star[v] = knn.bestk(X.learn,Y.learn)
  out.knn = knn(X.learn,X.val,Y.learn,k=k.star[v])
  
  # for log. reg
  data = data.frame(X = X.learn, Y = as.factor(Y.learn))
  colnames(data) = c(colnames(X.learn),"Y")
  f.lreg = glm(Y ~ ., data = data, family = "binomial")
  out.lreg = (predict(f.lreg,X.val,type = "response") >= 0.5) + 1
  
  # Evaluate the errors
  err.lda[v] = sum(out.lda$class != Y.val) / length(Y.val)
  err.knn[v] = sum(out.knn != Y.val) / length(Y.val)
  err.lreg[v] = sum(out.lreg != Y.val) / length(Y.val)
}
```

```{r}
cat("Err LDA:",mean(err.lda),"+/-",sd(err.lda),"\n")
cat("Err lreg:",mean(err.lreg),"+/-",sd(err.lreg),"\n")
cat("Err k-NN:",mean(err.knn),"+/-",sd(err.knn),"\n")
cat("Average k for k-NN:",median(k.star))
```


```{r}
Y.bin = as.numeric(Y)
Y.bin[Y.bin==2] = 1
Y.bin[Y.bin==3] = 2
```



```{r}
XX = X[,1:2]
x <- seq(min(XX[,1]),max(XX[,1]),length.out = 100)
y <- seq(min(XX[,2]),max(XX[,2]), length.out = 100)
S = expand.grid(x,y)
colnames(S) = colnames(XX)

f.lda = lda(XX,Y.bin)
out.lda = predict(f.lda,S)

plot(XX,type='n')
points(S,col=as.numeric(out.lda$class)+1,pch=19)
points(XX,col=1)

```


# GMM and EM algo
# unsupervised learning

```{r}
install.packages("mvtnorm")
```

```{r}
library(mvtnorm)
```


```{r}

EMalgo <- function(X, n.iter = 50){
  n = nrow()
  p = ncol()
  
  # initialization 
  prop = rep(1:K, K)
  mu = matrix(rnorm(K*p))
  Sigma =array(0, dim = c(K,p,p))
  
  for (k=1: K) Sigma[k,,]= diag(p)
  
  
  #The EM loop
  
  for (it in 1;n.iter){
    
    # E step
    for (k in 1:K){
      P[,k]=   dmvnorm(x, mean = mu, sigma = sigma)
    }
    
    # M step
    
    for (k in 1:K){
      prop[k]=
      mu[k,]=
      Sigma[k,,]=
    }
    
  }
  
  
  
  
}












```


