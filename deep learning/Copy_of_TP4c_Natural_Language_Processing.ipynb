{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/prabal5ghosh/UCA-M2-SEMESTER1/blob/main/deep%20learning/Copy_of_TP4c_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Udoa-p6dKy"
   },
   "source": [
    "<center><h1>TP4c: Natural Language Processing in Pytorch</h1></center>\n",
    "\n",
    "# Warning :\n",
    "# \"File -> Save a copy in Drive\" before starting to modify the notebook, otherwise changes won't be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCKoaBnwNehk",
    "outputId": "c0044f4c-df9e-4cb7-be93-cf08c5a411f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=4902930269e85c5122862cfcbd3b23c22dfdf0edbd7b50012d4dc3e6239c8581\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "! pip install wget\n",
    "! pip install seaborn\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DBJhNHi4u9_"
   },
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We will download the data from [here](https://download.pytorch.org/tutorial/data.zip) and extract it to the current directory.\n",
    "\n",
    "Included in the ``data/names`` directory there will be 18 text files named as\n",
    "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QK1ZZmZFNeho",
    "outputId": "0618f669-9ca0-4da3-a4a1-57b45af5d45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, root=\".\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize letter dictionary\n",
    "        self.all_letters = string.ascii_letters\n",
    "        self.n_letters = len(self.all_letters)\n",
    "\n",
    "        # download dataset\n",
    "        self._download_dataset(root)\n",
    "        self.root = os.path.join(root, \"data\", \"names\")\n",
    "\n",
    "        # Build the category_lines dictionary, a list of names per language\n",
    "        self.lines = []\n",
    "        self.labels = []\n",
    "        self.classes = []\n",
    "        for i, filename in enumerate(os.listdir(self.root)):\n",
    "            category = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.classes.append(category)\n",
    "            lines = self._readLines(filename)\n",
    "            self.lines.extend(lines)\n",
    "            self.labels.extend([i] * len(lines))\n",
    "\n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "\n",
    "    # Couple of utils functions\n",
    "    # Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "    def _unicodeToAscii(self, s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in self.all_letters\n",
    "        )\n",
    "\n",
    "    # Read lines from a file and and convert them to ASCII\n",
    "    def _readLines(self, filename):\n",
    "        path = os.path.join(self.root, filename)\n",
    "        lines = open(path, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [self._unicodeToAscii(line) for line in lines]\n",
    "\n",
    "    # Private function to download the dataset\n",
    "    def _download_dataset(self, root):\n",
    "        import shutil\n",
    "        import wget\n",
    "        import zipfile\n",
    "        if os.path.exists(root) and root != \".\":\n",
    "            shutil.rmtree(root) # make sure there is nothing in our folder\n",
    "\n",
    "        if root != \".\":\n",
    "            os.makedirs(root) # create folders\n",
    "        wget.download(\"https://download.pytorch.org/tutorial/data.zip\",\n",
    "                      root) # download dataset\n",
    "        with zipfile.ZipFile(os.path.join(root, \"data.zip\"), 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\") # extract dataset\n",
    "\n",
    "\n",
    "\n",
    "ds = NamesDataset(\".\")\n",
    "\n",
    "print(ds._unicodeToAscii('Ślusàrski'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_s0YkUdNehp"
   },
   "source": [
    "Now we have a dataset, with the complete list of names in ``ds.data`` and the corresponding list of language index ``ds.labels``.\n",
    "\n",
    "We also kept track of ``ds.classes`` (just a list of languages) and ``ds.n_classes`` (number of classes).\n",
    "<!-- and ``ds.max_lenght`` (max word length) for later reference. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ai2dFikkNehq",
    "outputId": "d7735b05-33eb-4e2b-fad6-024a9a68aae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abe', 'Abukara', 'Adachi', 'Aida', 'Aihara']\n",
      "[0, 0, 0, 0, 0]\n",
      "Number of classes: 18, Classes: ['Japanese', 'Polish', 'Russian', 'Irish', 'Italian', 'Scottish', 'Vietnamese', 'English', 'Czech', 'Greek', 'Dutch', 'Arabic', 'Chinese', 'French', 'Portuguese', 'Korean', 'Spanish', 'German']\n"
     ]
    }
   ],
   "source": [
    "print(ds.lines[:5])\n",
    "print(ds.labels[:5])\n",
    "print(f\"Number of classes: {ds.n_classes}, Classes: {ds.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avu-OkFvNehq"
   },
   "source": [
    "### Turning Names into Tensors\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we will use a \"one-hot vector\" of size\n",
    "``<n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "To make a word we join a bunch of those into a 3D matrix\n",
    "``<line_length x n_letters>``.\n",
    "\n",
    "For words the one-hot econding representation is also called *Bag of Words*. However, to have richer semantic representation of the words, several encoding algorithms have been developed, the most famous of which is *Word2Vec*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEmq2V1bNehq",
    "outputId": "75c8c05c-7f03-434d-da4f-4c56478762b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation of the words 'Phil'\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class NamesDataset(NamesDataset):\n",
    "\n",
    "    # Find letter index from all_letters, e.g. \"a\" = 0\n",
    "    def letter_2_index(self, letter):\n",
    "        return self.all_letters.find(letter)\n",
    "\n",
    "    # Turn a line into a <line_length  x n_letters>,\n",
    "    # or an array of one-hot letter vectors\n",
    "    def word_2_tensor(self, line):\n",
    "        tensor = torch.zeros(len(line), self.n_letters)\n",
    "        for i, letter in enumerate(line):\n",
    "            tensor[i][self.letter_2_index(letter)] = 1\n",
    "        return tensor\n",
    "\n",
    "ds = NamesDataset()\n",
    "print(\"Representation of the words 'Phil'\")\n",
    "print(ds.word_2_tensor('Phil'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEns-u6wNehr"
   },
   "source": [
    "## Creating the Network\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "We will employ a very simple RNN model with just 2 linear layers which operate on an input and hidden state, and a LogSoftmax layer after the output. The recursion is provided by the only hidden state which is held in memory.\n",
    "\n",
    "![](https://i.imgur.com/Z2xbySO.png)\n",
    "\n",
    "<!-- .. figure:: https://i.imgur.com/Z2xbySO.png\n",
    "   :alt: -->\n",
    "More specifically, in yellow are displayed tensors, in blue are displayed linear weight layers and in light blue the activation function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX4Qif74Nehr"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        ####################\n",
    "        ## Your code here ##\n",
    "        ####################\n",
    "\n",
    "        # Init hidden state\n",
    "        self.init_hidden()\n",
    "        # Create Linear layers\n",
    "        self.i2o = torch.nn.Linear(input_size +hidden_size ,output_size)\n",
    "\n",
    "        self.i2h = torch.nn.Linear(input_size+ hidden_size, hidden_size)\n",
    "        # Create activation function\n",
    "        self.activation = torch.nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # Concatenate input and current hidden state\n",
    "        concat = torch.cat([input, self.hidden],dim = -1)\n",
    "\n",
    "        # Pass through the two hidden layers the concatenation\n",
    "\n",
    "        i2o = self.i2o(concat)\n",
    "        hidden = self.i2h(concat)\n",
    "\n",
    "        # Save the hidden state for later\n",
    "        self.hidden = hidden\n",
    "\n",
    "\n",
    "\n",
    "        # Compute the output\n",
    "        output = self.activation(i2o)\n",
    "\n",
    "        ####################\n",
    "        ##      FIN        #\n",
    "        ####################\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self): # to reset hidden state\n",
    "        self.hidden = torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(ds.n_letters, n_hidden, ds.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf5Ik7WNNehs"
   },
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter). At first the hidden state will be\n",
    "initialized as zeros. We'll get back the output (probability of\n",
    "each language). The network will store the hidden state for the next\n",
    "step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBHGSwakNehs",
    "outputId": "8f4a137b-9c76-4dc6-db9f-53d02a855ba6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9869, -2.8522, -2.8063, -2.9414, -2.8496, -2.9390, -2.8396, -2.9778,\n",
       "         -2.9080, -2.8292, -2.9201, -2.8563, -2.9029, -2.9147, -2.9092, -2.8438,\n",
       "         -2.8851, -2.8866]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = ds.word_2_tensor('Angel').unsqueeze(dim=0)\n",
    "\n",
    "output = rnn(input[:, 0]) # 0 to select the first letter of the word\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOKuKfAcNeht"
   },
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "You may think that this output is meaningless beacuse the network is not trained yet. However, even after training, reading the output of the network after the first letter does not make sense! We *always* need to finish processing the entire word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIEPCJ5oNehu"
   },
   "source": [
    "## Training\n",
    "----------------------\n",
    "We will also want a quick way to get a training example (a name and its\n",
    "language). To do so we will further extend our ``NamesDataset`` class to have a ``__get_item__`` and a ``__len__`` function that we will use to index the dataset and get the # samples.\n",
    "\n",
    "Together with a ``DataLoader``, they will allows us to loop over the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZvHKa5lNehu",
    "outputId": "d9f8e919-993e-469f-9e77-8716541f419e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Japanese / name = Abe\n",
      "category = Japanese / name = Abukara\n",
      "category = Japanese / name = Adachi\n",
      "category = Japanese / name = Aida\n",
      "category = Japanese / name = Aihara\n",
      "category = Japanese / name = Aizawa\n",
      "category = Japanese / name = Ajibana\n",
      "category = Japanese / name = Akaike\n",
      "category = Japanese / name = Akamatsu\n",
      "category = Japanese / name = Akatsuka\n",
      "Number of samples: 20074\n"
     ]
    }
   ],
   "source": [
    "from abc import abstractclassmethod\n",
    "import random\n",
    "\n",
    "class NamesDataset(NamesDataset):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.lines[index]\n",
    "        x = self.word_2_tensor(line)\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def tensor_2_word(self, x):\n",
    "        word = []\n",
    "        for x_i in x:\n",
    "            idx = x_i.argmax()\n",
    "            letter = ds.all_letters[idx]\n",
    "            word.append(letter)\n",
    "        return ''.join(word)\n",
    "\n",
    "\n",
    "ds = NamesDataset()\n",
    "for i in range(10):\n",
    "    x, y = ds[i]\n",
    "    print('category =', ds.classes[y], '/ name =', ds.tensor_2_word(x))\n",
    "print(f\"Number of samples: {len(ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T0CEJQ6Nehu"
   },
   "source": [
    "### Training the Network\n",
    "\n",
    "We forget to create the training and test data. Let's use the ``random_split`` function from pytorch this time. It will create two ``Subset`` of our datasets with the required amount of training and test samples.\n",
    "\n",
    "For the loss function ``nn.NLLLoss`` is still appropriate, since the last\n",
    "layer of the RNN is ``nn.LogSoftmax``.\n",
    "\n",
    "Notice: we're just using a **batch_size = 1** here. This is beacause all words have different length and therefore we cannot directly perform batch prediction. It would require padding the input to a same dimensions and masking the output to get the correct final prediction: for the sake of simplicity we avoid doing it here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPHAoLPcNehv",
    "outputId": "c38e6c71-acb1-4537-e6d2-2a8934d7decf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train samples 18066, # val samples 2008\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_samples, val_samples = int(0.9*len(ds)), len(ds)-int(0.9*len(ds))\n",
    "train_ds, val_ds = random_split(ds, (train_samples, val_samples))\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "rnn = RNN(ds.n_letters, n_hidden, ds.n_classes)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"# Train samples {train_samples}, # val samples {val_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEuJtMqrNehv"
   },
   "source": [
    "### Training step:\n",
    "In this training step we have to:\n",
    "\n",
    "-  Read each letter in the input $x$\n",
    "    - **watch out**: $x$ will be now ``<# samples, # letters, input size>``\n",
    "    - to loop on the letter you have to index the 2nd dimension\n",
    "    - batch size = 1, so we only one deal with one sample at a time\n",
    "-  Compare final output to target\n",
    "-  Back-propagate\n",
    "-  Re-initialize the hidden state\n",
    "-  Return the output and loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GC1OMx-zNehv"
   },
   "outputs": [],
   "source": [
    "def train_step(rnn, x, y, criterion, optim):\n",
    "    ####################\n",
    "    ## Your code here ##\n",
    "    ####################\n",
    "\n",
    "    # Read each letter and pass it through the network\n",
    "\n",
    "\n",
    "    # Compute loss and backprop\n",
    "\n",
    "\n",
    "    ####################\n",
    "    ##      FIN        #\n",
    "    ####################\n",
    "\n",
    "    # Re-initialize hidden state\n",
    "    rnn.init_hidden()\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaiEFPIwIlmW"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, character_encoder):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "\n",
    "        self.character_encoder = character_encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Read each letter and pass it through the network\n",
    "        for i in range(x.size()[1]):\n",
    "          output = self.character_encoder(x[:,i])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHu0Hukhfswn"
   },
   "source": [
    "## Evaluation step\n",
    "In this evaluation step we only have to the same things as above but without backpropagating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGZc03Qxf0Nj"
   },
   "outputs": [],
   "source": [
    "def eval_step(rnn, x, y, criterion):\n",
    "\n",
    "    # Read each letter and it through the network\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        output = rnn(x[:, i])\n",
    "\n",
    "    # Compute loss and backprop\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    # Re-initialize hidden state\n",
    "    rnn.init_hidden()\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUOA-vKbNehv"
   },
   "source": [
    "Since the ``train`` function returns both the output and loss we can print its\n",
    "guesses and also keep track of loss and accuracy for plotting.\n",
    "\n",
    "We don't use gpu this time since we are not performing batch computation and input and net dimension are very small. The complete training should take about 5 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAs71LN5Nehw"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "# Keep track of losses and accs\n",
    "df = {\n",
    "    \"epoch\": [],\n",
    "    \"loss\": [],\n",
    "    \"acc\": [],\n",
    "    \"mode\": [],\n",
    "}\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "best_acc = 0.\n",
    "for e in range(epochs):\n",
    "    train_loss, train_acc = 0, 0.\n",
    "    for x, y in train_dl:\n",
    "        output, loss = train_step(rnn, x, y, criterion, optimizer)\n",
    "        train_loss += loss\n",
    "        train_acc += output.argmax() == y\n",
    "\n",
    "    # Store train avg epoch loss and accuracy\n",
    "    df[\"loss\"].append(train_loss / train_samples)\n",
    "    df[\"acc\"].append(train_acc.item() / train_samples)\n",
    "    df[\"mode\"].append(\"train\"), df[\"epoch\"].append(e)\n",
    "\n",
    "    val_loss, val_acc = 0, 0.\n",
    "    for x, y in val_dl:\n",
    "        output, loss = eval_step(rnn, x, y, criterion)\n",
    "        val_loss += loss\n",
    "        val_acc += output.argmax() == y\n",
    "\n",
    "    if best_acc < val_acc / val_samples:\n",
    "        best_acc = val_acc / val_samples\n",
    "        torch.save(rnn.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    # Store val avg epoch loss and accuracy\n",
    "    df[\"loss\"].append(val_loss / val_samples)\n",
    "    df[\"acc\"].append(val_acc.item() / val_samples)\n",
    "    df[\"mode\"].append(\"val\"), df[\"epoch\"].append(e)\n",
    "\n",
    "    # Print epoch number, loss, name and guess\n",
    "    class_predict = ds.classes[output.argmax()]\n",
    "    print('%d/ %d (%s) %.2f %.2f %.2f %.2f' % (e + 1, epochs, timeSince(start),\n",
    "                                                train_loss / train_samples,\n",
    "                                                train_acc / train_samples,\n",
    "                                                val_loss / val_samples,\n",
    "                                                val_acc / val_samples))\n",
    "rnn.load_state_dict(torch.load(\"best_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upAkeLDwNehw"
   },
   "source": [
    "### Plotting the Results\n",
    "\n",
    "Plotting the historical loss and accuracy from ``df`` to shows the network\n",
    "learning both at train and at test time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaVORjXINehw"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "df_pd = pd.DataFrame(df)\n",
    "sns.lineplot(data=df_pd, x=\"epoch\", y='loss', hue=\"mode\")\n",
    "plt.show()\n",
    "sns.lineplot(data=df_pd, x=\"epoch\", y='acc', hue=\"mode\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC5SIoplNehx"
   },
   "source": [
    "### Running on User Input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIPBSYL4Nehx"
   },
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        x = ds.word_2_tensor(input_line).unsqueeze(0)\n",
    "        for i in range(x.shape[1]):\n",
    "            output = rnn(x[:, i])\n",
    "        rnn.init_hidden()\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, ds.classes[category_index]))\n",
    "            predictions.append([value, ds.classes[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BUyMk0DNehx"
   },
   "source": [
    "## Bonus exercises\n",
    "\n",
    "-  Get better results with a bigger and/or better shaped network\n",
    "\n",
    "   -  Add more linear layers\n",
    "   -  Try the ``nn.LSTM`` and ``nn.GRU`` layers\n",
    "   -  Combine multiple of these RNNs as a higher level network\n",
    "\n",
    "\n",
    "-  Try with a different dataset of line -> category, for example:\n",
    "\n",
    "   -  Any word -> language\n",
    "   -  First name -> gender\n",
    "   -  Character name -> writer\n",
    "   -  Page title -> blog or subreddit\n",
    "\n",
    "You can find many easy-to-use datasets in [scikit-learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64rbZ7CFO0S4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
