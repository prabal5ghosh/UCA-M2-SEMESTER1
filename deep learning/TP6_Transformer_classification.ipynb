{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabal5ghosh/UCA-M2-SEMESTER1/blob/main/deep%20learning/TP6_Transformer_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>TP6: Classification with Transformers</h1></center>\n",
        "\n",
        "# Warning :\n",
        "# \"File -> Save a copy in Drive\" before starting to modify the notebook, otherwise changes won't be saved."
      ],
      "metadata": {
        "id": "utLsYPhXT0Jz"
      },
      "id": "utLsYPhXT0Jz"
    },
    {
      "cell_type": "markdown",
      "id": "acbeb1c6",
      "metadata": {
        "papermill": {
          "duration": 0.011303,
          "end_time": "2022-04-09T14:37:30.875464",
          "exception": false,
          "start_time": "2022-04-09T14:37:30.864161",
          "status": "completed"
        },
        "tags": [],
        "id": "acbeb1c6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3788f0d0",
      "metadata": {
        "papermill": {
          "duration": 0.011526,
          "end_time": "2022-04-09T14:37:34.434868",
          "exception": false,
          "start_time": "2022-04-09T14:37:34.423342",
          "status": "completed"
        },
        "tags": [],
        "id": "3788f0d0"
      },
      "source": [
        "\n",
        "Below, we import some standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c0e503",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:34.458897Z",
          "iopub.status.busy": "2022-04-09T14:37:34.458462Z",
          "iopub.status.idle": "2022-04-09T14:37:37.002927Z",
          "shell.execute_reply": "2022-04-09T14:37:37.002262Z"
        },
        "papermill": {
          "duration": 2.558559,
          "end_time": "2022-04-09T14:37:37.004560",
          "exception": false,
          "start_time": "2022-04-09T14:37:34.446001",
          "status": "completed"
        },
        "tags": [],
        "id": "58c0e503"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "from functools import partial\n",
        "from urllib.error import HTTPError\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "# Plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"data/\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"saved_models/\"\n",
        "\n",
        "# Set seed to ensure that all operations are deterministic for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember the scaled dot product? Well, here is a refresher exercise!"
      ],
      "metadata": {
        "id": "Yua1mKAVVM3L"
      },
      "id": "Yua1mKAVVM3L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9070779d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.447403Z",
          "iopub.status.busy": "2022-04-09T14:37:37.446989Z",
          "iopub.status.idle": "2022-04-09T14:37:37.451657Z",
          "shell.execute_reply": "2022-04-09T14:37:37.451096Z"
        },
        "papermill": {
          "duration": 0.018657,
          "end_time": "2022-04-09T14:37:37.453028",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.434371",
          "status": "completed"
        },
        "tags": [],
        "id": "9070779d"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "\n",
        "    #######################\n",
        "    ### YOUR CODE HERE! ###\n",
        "    #######################\n",
        "\n",
        "    # Compute attn_logits\n",
        "    attn_logits = None\n",
        "\n",
        "    # Apply mask if not None\n",
        "    if mask is not None:\n",
        "        attn_logits = None\n",
        "\n",
        "    # Pass through softmax\n",
        "    attention = None\n",
        "\n",
        "    # Weight values accordingly\n",
        "    output_values = None\n",
        "\n",
        "    #######################\n",
        "    ###       END       ###\n",
        "    #######################\n",
        "\n",
        "    return output_values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did we deal with the tricky shapes of MultiheadAttention?"
      ],
      "metadata": {
        "id": "4SpqdXL2VZEB"
      },
      "id": "4SpqdXL2VZEB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa28f1cf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.586727Z",
          "iopub.status.busy": "2022-04-09T14:37:37.586295Z",
          "iopub.status.idle": "2022-04-09T14:37:37.594515Z",
          "shell.execute_reply": "2022-04-09T14:37:37.593932Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.022557,
          "end_time": "2022-04-09T14:37:37.595903",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.573346",
          "status": "completed"
        },
        "tags": [],
        "id": "fa28f1cf"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim # dimension of concatenated heads\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.qkv_proj = nn.Linear(input_dim, embed_dim * 3)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "\n",
        "        #######################\n",
        "        ### YOUR CODE HERE! ###\n",
        "        #######################\n",
        "\n",
        "        batch_dim, seq_length, input_dim = x.shape\n",
        "\n",
        "        # Compute linear projection for qkv and separate heads\n",
        "        # QKV: [Batch, Head, SeqLen, Dims]\n",
        "        qkv = None\n",
        "        q, k, v = None\n",
        "\n",
        "\n",
        "        # Apply Dot Product Attention to qkv ()\n",
        "        attention_values, attention = None\n",
        "\n",
        "        # Concatenate heads to [Batch, SeqLen, Embed Dim]\n",
        "        attention_values = None\n",
        "\n",
        "        # Output projection\n",
        "        o = None\n",
        "\n",
        "        #######################\n",
        "        ###       END       ###\n",
        "        #######################\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o\n",
        "\n",
        "input_d = 3\n",
        "seq_l = 4\n",
        "embed_d = 4\n",
        "n_heads = 2\n",
        "b_size = 1\n",
        "\n",
        "mh_att = MultiheadAttention(input_d, embed_d, n_heads)\n",
        "\n",
        "x = torch.rand(b_size, seq_l, input_d)\n",
        "x = torch.tensor([[[0.3360, 0.6676, 0.6393],\n",
        "         [0.2083, 0.5484, 0.1204],\n",
        "         [0.3533, 0.3038, 0.9383],\n",
        "         [0.0499, 0.2048, 0.0107]]])\n",
        "print(f\"Input x: {x}\")\n",
        "\n",
        "att_output = mh_att(x)\n",
        "print(f\"MhA Output {att_output}\")\n",
        "assert att_output.shape == torch.Size([1, 4, 4]), \"Error in computing multi-head attention\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55bfef0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.670726Z",
          "iopub.status.busy": "2022-04-09T14:37:37.670245Z",
          "iopub.status.idle": "2022-04-09T14:37:37.676639Z",
          "shell.execute_reply": "2022-04-09T14:37:37.676067Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.020505,
          "end_time": "2022-04-09T14:37:37.678049",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.657544",
          "status": "completed"
        },
        "tags": [],
        "id": "c55bfef0"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: Dimensionality of the input\n",
        "            num_heads: Number of heads to use in the attention block\n",
        "            dim_feedforward: Dimensionality of the hidden layer in the MLP\n",
        "            dropout: Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Create Two-layer MLP with droput\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(2*input_dim, input_dim)\n",
        "        )\n",
        "        # Layers to apply in between the main layers (Layer Norm and Dropout)\n",
        "        self.norm = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Compute Attention part\n",
        "        attn=self.self_attn(x)\n",
        "        x=self.norm(attn+x)\n",
        "\n",
        "        # Compute MLP part\n",
        "        x = self.norm(x+self.mlp(x))\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e4943c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.727879Z",
          "iopub.status.busy": "2022-04-09T14:37:37.726612Z",
          "iopub.status.idle": "2022-04-09T14:37:37.733398Z",
          "shell.execute_reply": "2022-04-09T14:37:37.732829Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.020849,
          "end_time": "2022-04-09T14:37:37.734822",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.713973",
          "status": "completed"
        },
        "tags": [],
        "id": "59e4943c"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for layer in self.layers:\n",
        "            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = layer(x)\n",
        "        return attention_maps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last time we used a pre-computed sine positional encoding, which is often used to this day.\n",
        "\n",
        "This time we will see the other common positional encoding type: learned positional encodings! Initialize random embeddings for each possible position and make sure these embeddings are tracked by the model!"
      ],
      "metadata": {
        "id": "Y812san3j78o"
      },
      "id": "Y812san3j78o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97458a86",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:37.784031Z",
          "iopub.status.busy": "2022-04-09T14:37:37.783642Z",
          "iopub.status.idle": "2022-04-09T14:37:37.789832Z",
          "shell.execute_reply": "2022-04-09T14:37:37.789242Z"
        },
        "papermill": {
          "duration": 0.02026,
          "end_time": "2022-04-09T14:37:37.791228",
          "exception": false,
          "start_time": "2022-04-09T14:37:37.770968",
          "status": "completed"
        },
        "tags": [],
        "id": "97458a86"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        \"\"\"\n",
        "        Args\n",
        "            d_model: Hidden dimensionality of the input.\n",
        "            max_len: Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        #######################\n",
        "        ### YOUR CODE HERE! ###\n",
        "        #######################\n",
        "\n",
        "        # Create random matrix of [1, SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        self.pe = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = None\n",
        "\n",
        "        #######################\n",
        "        ###       END       ###\n",
        "        #######################\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba3970c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.307736Z",
          "iopub.status.busy": "2022-04-09T14:37:40.307446Z",
          "iopub.status.idle": "2022-04-09T14:37:40.319189Z",
          "shell.execute_reply": "2022-04-09T14:37:40.318619Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.031287,
          "end_time": "2022-04-09T14:37:40.320713",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.289426",
          "status": "completed"
        },
        "tags": [],
        "id": "fba3970c"
      },
      "outputs": [],
      "source": [
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        model_dim,\n",
        "        num_classes,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        dropout=0.0,\n",
        "        input_dropout=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: Hidden dimensionality of the input\n",
        "            model_dim: Hidden dimensionality to use inside the Transformer\n",
        "            num_classes: Number of classes to predict per sequence element\n",
        "            num_heads: Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers: Number of encoder blocks to use.\n",
        "            lr: Learning rate in the optimizer\n",
        "            warmup: Number of warmup steps. Usually between 50 and 500\n",
        "            max_iters: Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
        "            dropout: Dropout to apply inside the model\n",
        "            input_dropout: Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.model_dim = model_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.input_dropout = input_dropout\n",
        "\n",
        "        # Create a Generic Input Encoder Input dim -> Model dim with input dropout\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, model_dim),\n",
        "            nn.Dropout(input_dropout)\n",
        "        )\n",
        "\n",
        "        # Create positional encoding for sequences\n",
        "        self.positional_encoding = PositionalEncoding(model_dim)\n",
        "\n",
        "        # Create transformer Encoder\n",
        "        self.transformer = TransformerEncoder(num_layers, input_dim=model_dim, dim_feedforward=model_dim*2, num_heads=num_heads, dropout=dropout)\n",
        "\n",
        "        # Create output classifier per sequence element Model_dim -> num_classes\n",
        "        self.output_net = nn.Linear(model_dim, num_classes)\n",
        "\n",
        "        #######################\n",
        "        ### YOUR CODE HERE! ###\n",
        "        #######################\n",
        "\n",
        "        # Create classification token\n",
        "        self.cls_token = None\n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input features of shape [Batch, SeqLen, input_dim]\n",
        "            mask: Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding: If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "\n",
        "        # Add the cls token\n",
        "        x = None\n",
        "\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = None\n",
        "\n",
        "        # Get the output! Remember we only care about the classification token!\n",
        "        x = None\n",
        "\n",
        "        #######################\n",
        "        ###       END       ###\n",
        "        #######################\n",
        "\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        x = torch.cat([self.cls_token.expand(x.shape[0],-1,-1), x], dim=1)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f853d7",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017029,
          "end_time": "2022-04-09T14:37:40.355127",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.338098",
          "status": "completed"
        },
        "tags": [],
        "id": "e2f853d7"
      },
      "source": [
        "## Experiment: Sequence Classification\n",
        "\n",
        "Let's try to do some classification with a simple task.\n",
        "\n",
        "The following implements a dataset that counts the number of 0s in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678b995e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.390306Z",
          "iopub.status.busy": "2022-04-09T14:37:40.389769Z",
          "iopub.status.idle": "2022-04-09T14:37:40.394655Z",
          "shell.execute_reply": "2022-04-09T14:37:40.394092Z"
        },
        "papermill": {
          "duration": 0.023865,
          "end_time": "2022-04-09T14:37:40.396024",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.372159",
          "status": "completed"
        },
        "tags": [],
        "id": "678b995e"
      },
      "outputs": [],
      "source": [
        "class ZeroCountDataset(data.Dataset):\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(10, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.sum(inp_data == 0)\n",
        "        return inp_data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c57b9a",
      "metadata": {
        "papermill": {
          "duration": 0.017261,
          "end_time": "2022-04-09T14:37:40.430322",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.413061",
          "status": "completed"
        },
        "tags": [],
        "id": "d5c57b9a"
      },
      "source": [
        "We create an arbitrary number of random sequences of numbers between 0 and `num_categories-1`.\n",
        "The label is simply the number of 0s.\n",
        "We can create the corresponding data loaders below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d155c78",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.465826Z",
          "iopub.status.busy": "2022-04-09T14:37:40.465300Z",
          "iopub.status.idle": "2022-04-09T14:37:40.483277Z",
          "shell.execute_reply": "2022-04-09T14:37:40.482703Z"
        },
        "papermill": {
          "duration": 0.037365,
          "end_time": "2022-04-09T14:37:40.484763",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.447398",
          "status": "completed"
        },
        "tags": [],
        "id": "7d155c78"
      },
      "outputs": [],
      "source": [
        "dataset = partial(ZeroCountDataset, 16, 16)\n",
        "train_dl = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_dl = data.DataLoader(dataset(1000), batch_size=128)\n",
        "test_dl = data.DataLoader(dataset(10000), batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108b3cc8",
      "metadata": {
        "papermill": {
          "duration": 0.017281,
          "end_time": "2022-04-09T14:37:40.519172",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.501891",
          "status": "completed"
        },
        "tags": [],
        "id": "108b3cc8"
      },
      "source": [
        "Let's look at an arbitrary sample of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54622080",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.554479Z",
          "iopub.status.busy": "2022-04-09T14:37:40.553970Z",
          "iopub.status.idle": "2022-04-09T14:37:40.558389Z",
          "shell.execute_reply": "2022-04-09T14:37:40.557790Z"
        },
        "papermill": {
          "duration": 0.023559,
          "end_time": "2022-04-09T14:37:40.559783",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.536224",
          "status": "completed"
        },
        "tags": [],
        "id": "54622080"
      },
      "outputs": [],
      "source": [
        "inp_data, labels = train_dl.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3157b207",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.629517Z",
          "iopub.status.busy": "2022-04-09T14:37:40.629034Z",
          "iopub.status.idle": "2022-04-09T14:37:40.635426Z",
          "shell.execute_reply": "2022-04-09T14:37:40.634853Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.025464,
          "end_time": "2022-04-09T14:37:40.636801",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.611337",
          "status": "completed"
        },
        "tags": [],
        "id": "3157b207"
      },
      "outputs": [],
      "source": [
        "def train_step(model, x, y, optim):\n",
        "    model.train()\n",
        "\n",
        "    # Fetch data and transform categories to one-hot vectors\n",
        "    inp_data = F.one_hot(x, num_classes=10).float()\n",
        "\n",
        "    # Perform prediction and calculate loss and accuracy\n",
        "    preds = model(inp_data, add_positional_encoding=True)\n",
        "    loss = F.cross_entropy(preds.view(-1, preds.size(-1)), y.view(-1))\n",
        "    acc = (preds.argmax(dim=-1) == y).float().mean()\n",
        "\n",
        "    # Backpropagate and update weights\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def eval_step(model, x, y):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        # Fetch data and transform categories to one-hot vectors\n",
        "        inp_data = F.one_hot(x, num_classes=model.num_classes).float()\n",
        "\n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        preds = model(inp_data, add_positional_encoding=True)\n",
        "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), y.view(-1))\n",
        "        acc = (preds.argmax(dim=-1) == y).float().mean()\n",
        "\n",
        "    return loss, acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eca170b",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017272,
          "end_time": "2022-04-09T14:37:40.673633",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.656361",
          "status": "completed"
        },
        "tags": [],
        "id": "1eca170b"
      },
      "source": [
        "Finally, we can create a training function similar to the one we have seen in previous laboratories. We running for $N$ epochs printing the training and validation loss and saving our best model based on the validation.\n",
        "Afterward, we test our models on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237c583b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.709513Z",
          "iopub.status.busy": "2022-04-09T14:37:40.708933Z",
          "iopub.status.idle": "2022-04-09T14:37:40.715633Z",
          "shell.execute_reply": "2022-04-09T14:37:40.715064Z"
        },
        "papermill": {
          "duration": 0.026011,
          "end_time": "2022-04-09T14:37:40.717000",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.690989",
          "status": "completed"
        },
        "tags": [],
        "id": "237c583b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, test_loader,\n",
        "                optim, epochs=5):\n",
        "    best_acc = 0.\n",
        "    pbar = tqdm(range(epochs))\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_acc = 0., 0.\n",
        "        for x, y in train_loader:\n",
        "            loss, acc = train_step(model, x, y, optim)\n",
        "            train_loss += loss\n",
        "            train_acc += acc\n",
        "\n",
        "        val_loss, val_acc = 0., 0.\n",
        "        for x, y in val_loader:\n",
        "            loss, acc = eval_step(model, x, y)\n",
        "            val_loss += loss\n",
        "            val_acc += acc\n",
        "\n",
        "        if val_acc/len(val_loader) > best_acc:\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            best_acc = val_acc/len(val_loader)\n",
        "\n",
        "        pbar.update()\n",
        "        pbar.set_description(f\"Train Acc: {train_acc/len(train_loader)* 100:.2f} \"\n",
        "                            f\"Train Loss: {train_loss/len(train_loader):.2f} \"\n",
        "                            f\"Val Acc: {val_acc/len(val_loader)* 100 :.2f}  \"\n",
        "                            f\"Val loss: {val_loss/len(val_loader):.2f} \")\n",
        "\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for x, y in test_loader:\n",
        "        loss, acc = eval_step(model, x, y)\n",
        "        test_loss += loss\n",
        "        test_acc += acc\n",
        "\n",
        "    print(f\"Test accuracy: {test_acc/len(test_loader)*100 :.2f}\")\n",
        "\n",
        "    pbar.close()\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170c6143",
      "metadata": {
        "papermill": {
          "duration": 0.017478,
          "end_time": "2022-04-09T14:37:40.752397",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.734919",
          "status": "completed"
        },
        "tags": [],
        "id": "170c6143"
      },
      "source": [
        "Finally, we can train the model.\n",
        "In this setup, we will use a single encoder block and a single head in the Multi-Head Attention.\n",
        "This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted\n",
        "as an \"explanation\" of the predictions (compared to the other papers above dealing with deep Transformers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da3e1d49",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.788473Z",
          "iopub.status.busy": "2022-04-09T14:37:40.787980Z",
          "iopub.status.idle": "2022-04-09T14:37:44.274530Z",
          "shell.execute_reply": "2022-04-09T14:37:44.273926Z"
        },
        "papermill": {
          "duration": 3.505945,
          "end_time": "2022-04-09T14:37:44.275998",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.770053",
          "status": "completed"
        },
        "tags": [],
        "id": "da3e1d49"
      },
      "outputs": [],
      "source": [
        "count0_model = TransformerPredictor(\n",
        "    input_dim=10,\n",
        "    model_dim=32,\n",
        "    num_heads=1,\n",
        "    num_classes=10,\n",
        "    num_layers=1,\n",
        "    dropout=0.0,\n",
        ")\n",
        "optimizer = optim.AdamW(count0_model.parameters(), lr=0.001)\n",
        "\n",
        "count0_model = train_model(count0_model, train_dl, val_dl, test_dl, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbab7649",
      "metadata": {
        "papermill": {
          "duration": 0.017964,
          "end_time": "2022-04-09T14:37:44.392392",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.374428",
          "status": "completed"
        },
        "tags": [],
        "id": "bbab7649"
      },
      "source": [
        "As we would have expected, the Transformer can correctly solve the task.\n",
        "However, how does the attention in the Multi-Head Attention block looks like for an arbitrary input?\n",
        "Let's try to visualize it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d2704d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:44.429196Z",
          "iopub.status.busy": "2022-04-09T14:37:44.428820Z",
          "iopub.status.idle": "2022-04-09T14:37:44.436248Z",
          "shell.execute_reply": "2022-04-09T14:37:44.435661Z"
        },
        "papermill": {
          "duration": 0.027384,
          "end_time": "2022-04-09T14:37:44.437643",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.410259",
          "status": "completed"
        },
        "tags": [],
        "id": "c8d2704d"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "### YOUR CODE HERE! ###\n",
        "#######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017029,
          "end_time": "2022-04-09T14:37:40.355127",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.338098",
          "status": "completed"
        },
        "tags": [],
        "id": "i5wsTFNGioFo"
      },
      "source": [
        "## Bonus: 0 detection\n",
        "\n",
        "Let's try to do go further and detect the position and number of 0s in sentence that contains only one block of 0s.\n",
        "\n",
        "The following dataset implements sentences of digits that contains one block of 0s (between 1 and 4 consecutive 0s)."
      ],
      "id": "i5wsTFNGioFo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.390306Z",
          "iopub.status.busy": "2022-04-09T14:37:40.389769Z",
          "iopub.status.idle": "2022-04-09T14:37:40.394655Z",
          "shell.execute_reply": "2022-04-09T14:37:40.394092Z"
        },
        "papermill": {
          "duration": 0.023865,
          "end_time": "2022-04-09T14:37:40.396024",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.372159",
          "status": "completed"
        },
        "tags": [],
        "id": "DrcRshD2ioFr"
      },
      "outputs": [],
      "source": [
        "class ZeroDetectDataset(data.Dataset):\n",
        "    def __init__(self, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(1, 10, size=(self.size, self.seq_len))\n",
        "        self.starts = torch.randint(self.seq_len-4, size=(self.size,))\n",
        "        self.lengths = torch.randint(1, 4, size=(self.size,))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        inp_starts = self.starts[idx]\n",
        "        inp_lengths = self.lengths[idx]\n",
        "\n",
        "        inp_data[inp_starts:inp_starts+inp_lengths]=0\n",
        "        return inp_data, inp_starts, inp_lengths"
      ],
      "id": "DrcRshD2ioFr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017261,
          "end_time": "2022-04-09T14:37:40.430322",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.413061",
          "status": "completed"
        },
        "tags": [],
        "id": "r2xi7Sm9ioFs"
      },
      "source": [
        "We create an arbitrary number of random sequences of numbers between 0 and `num_categories-1`.\n",
        "The label is simply the number of 0s.\n",
        "We can create the corresponding data loaders below."
      ],
      "id": "r2xi7Sm9ioFs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.465826Z",
          "iopub.status.busy": "2022-04-09T14:37:40.465300Z",
          "iopub.status.idle": "2022-04-09T14:37:40.483277Z",
          "shell.execute_reply": "2022-04-09T14:37:40.482703Z"
        },
        "papermill": {
          "duration": 0.037365,
          "end_time": "2022-04-09T14:37:40.484763",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.447398",
          "status": "completed"
        },
        "tags": [],
        "id": "0lDfqGCFioFs"
      },
      "outputs": [],
      "source": [
        "dataset = partial(ZeroDetectDataset, 16)\n",
        "train_dl = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_dl = data.DataLoader(dataset(1000), batch_size=128)\n",
        "test_dl = data.DataLoader(dataset(10000), batch_size=128)"
      ],
      "id": "0lDfqGCFioFs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017281,
          "end_time": "2022-04-09T14:37:40.519172",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.501891",
          "status": "completed"
        },
        "tags": [],
        "id": "m9ElqgYHioFt"
      },
      "source": [
        "Let's look at an arbitrary sample of the dataset:"
      ],
      "id": "m9ElqgYHioFt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.554479Z",
          "iopub.status.busy": "2022-04-09T14:37:40.553970Z",
          "iopub.status.idle": "2022-04-09T14:37:40.558389Z",
          "shell.execute_reply": "2022-04-09T14:37:40.557790Z"
        },
        "papermill": {
          "duration": 0.023559,
          "end_time": "2022-04-09T14:37:40.559783",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.536224",
          "status": "completed"
        },
        "tags": [],
        "id": "c6R7ANuZioFu"
      },
      "outputs": [],
      "source": [
        "inp_data, inp_start, inp_length = train_dl.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Input start:\", inp_start)\n",
        "print(\"Input length:\", inp_length)"
      ],
      "id": "c6R7ANuZioFu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have two labels: the position of our \"0\" block, and the number of 0s to detect. This is basically detecting a bounding box in the sentence!\n",
        "\n",
        "To predict this, we are going to need to modify slightly the transformer model"
      ],
      "metadata": {
        "id": "UcV1Y7fkm7sg"
      },
      "id": "UcV1Y7fkm7sg"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        model_dim,\n",
        "        num_start,\n",
        "        num_length,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        dropout=0.0,\n",
        "        input_dropout=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: Hidden dimensionality of the input\n",
        "            model_dim: Hidden dimensionality to use inside the Transformer\n",
        "            num_classes: Number of classes to predict per sequence element\n",
        "            num_heads: Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers: Number of encoder blocks to use.\n",
        "            lr: Learning rate in the optimizer\n",
        "            warmup: Number of warmup steps. Usually between 50 and 500\n",
        "            max_iters: Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
        "            dropout: Dropout to apply inside the model\n",
        "            input_dropout: Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.model_dim = model_dim\n",
        "        self.num_start = num_start\n",
        "        self.num_length = num_length\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.input_dropout = input_dropout\n",
        "\n",
        "        # Create a Generic Input Encoder Input dim -> Model dim with input dropout\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, model_dim),\n",
        "            nn.Dropout(input_dropout)\n",
        "        )\n",
        "\n",
        "        # Create positional encoding for sequences\n",
        "        self.positional_encoding = PositionalEncoding(model_dim)\n",
        "\n",
        "        # Create transformer Encoder\n",
        "        self.transformer = TransformerEncoder(num_layers, input_dim=model_dim, dim_feedforward=model_dim*2, num_heads=num_heads, dropout=dropout)\n",
        "\n",
        "        #######################\n",
        "        ### YOUR CODE HERE! ###\n",
        "        #######################\n",
        "\n",
        "        # Create output classifier per sequence element Model_dim -> num_classes\n",
        "        self.start_net = None\n",
        "        self.length_net = None\n",
        "\n",
        "        # Create classification token\n",
        "        self.cls_token = None\n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input features of shape [Batch, SeqLen, input_dim]\n",
        "            mask: Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding: If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "\n",
        "        # Add the cls token\n",
        "        x = None\n",
        "\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = None\n",
        "\n",
        "        # Get the output! Remember we only care about the classification token!\n",
        "        start = None\n",
        "        length = None\n",
        "\n",
        "        #######################\n",
        "        ###       END       ###\n",
        "        #######################\n",
        "\n",
        "        return start, length\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        x = torch.cat([self.cls_token.expand(x.shape[0],-1,-1), x], dim=1)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n"
      ],
      "metadata": {
        "id": "j1k4yZL-nNxs"
      },
      "id": "j1k4yZL-nNxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.629517Z",
          "iopub.status.busy": "2022-04-09T14:37:40.629034Z",
          "iopub.status.idle": "2022-04-09T14:37:40.635426Z",
          "shell.execute_reply": "2022-04-09T14:37:40.634853Z"
        },
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.025464,
          "end_time": "2022-04-09T14:37:40.636801",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.611337",
          "status": "completed"
        },
        "tags": [],
        "id": "6sHYk2EpioFu"
      },
      "outputs": [],
      "source": [
        "def train_step(model, x, y_start, y_length, optim):\n",
        "    model.train()\n",
        "\n",
        "    # Fetch data and transform categories to one-hot vectors\n",
        "    inp_data = F.one_hot(x, num_classes=10).float()\n",
        "\n",
        "    # Perform prediction and calculate loss and accuracy\n",
        "    preds_start, preds_length = model(inp_data, add_positional_encoding=True)\n",
        "    loss = F.cross_entropy(preds_start.view(-1, preds_start.size(-1)), y_start.view(-1))\n",
        "    loss += F.cross_entropy(preds_length.view(-1, preds_length.size(-1)), y_length.view(-1))\n",
        "\n",
        "    acc = (preds_start.argmax(dim=-1) == y_start).float().mean()\n",
        "    acc += (preds_length.argmax(dim=-1) == y_length).float().mean()\n",
        "\n",
        "    # Backpropagate and update weights\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    return loss, acc/2\n",
        "\n",
        "def eval_step(model, x, y_start, y_length):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        # Fetch data and transform categories to one-hot vectors\n",
        "        inp_data = F.one_hot(x, num_classes=10).float()\n",
        "\n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        preds_start, preds_length = model(inp_data, add_positional_encoding=True)\n",
        "        loss = F.cross_entropy(preds_start.view(-1, preds_start.size(-1)), y_start.view(-1))\n",
        "        loss += F.cross_entropy(preds_length.view(-1, preds_length.size(-1)), y_length.view(-1))\n",
        "\n",
        "        acc = (preds_start.argmax(dim=-1) == y_start).float().mean()\n",
        "        acc += (preds_length.argmax(dim=-1) == y_length).float().mean()\n",
        "\n",
        "    return loss, acc/2\n"
      ],
      "id": "6sHYk2EpioFu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 2,
        "papermill": {
          "duration": 0.017272,
          "end_time": "2022-04-09T14:37:40.673633",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.656361",
          "status": "completed"
        },
        "tags": [],
        "id": "yZDD6RX8ioFv"
      },
      "source": [
        "Finally, we can create a training function similar to the one we have seen in previous laboratories. We running for $N$ epochs printing the training and validation loss and saving our best model based on the validation.\n",
        "Afterward, we test our models on the test set."
      ],
      "id": "yZDD6RX8ioFv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.709513Z",
          "iopub.status.busy": "2022-04-09T14:37:40.708933Z",
          "iopub.status.idle": "2022-04-09T14:37:40.715633Z",
          "shell.execute_reply": "2022-04-09T14:37:40.715064Z"
        },
        "papermill": {
          "duration": 0.026011,
          "end_time": "2022-04-09T14:37:40.717000",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.690989",
          "status": "completed"
        },
        "tags": [],
        "id": "MEzuAKHrioFv"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, test_loader,\n",
        "                optim, epochs=10):\n",
        "    best_acc = 0.\n",
        "    pbar = tqdm(range(epochs))\n",
        "    for e in range(epochs):\n",
        "        train_loss, train_acc = 0., 0.\n",
        "        for x, y_start, y_length in train_loader:\n",
        "            loss, acc = train_step(model, x, y_start, y_length, optim)\n",
        "            train_loss += loss\n",
        "            train_acc += acc\n",
        "\n",
        "        val_loss, val_acc = 0., 0.\n",
        "        for x, y_start, y_length in val_loader:\n",
        "            loss, acc = eval_step(model, x, y_start, y_length)\n",
        "            val_loss += loss\n",
        "            val_acc += acc\n",
        "\n",
        "        if val_acc/len(val_loader) > best_acc:\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            best_acc = val_acc/len(val_loader)\n",
        "\n",
        "        pbar.update()\n",
        "        pbar.set_description(f\"Train Acc: {train_acc/len(train_loader)* 100:.2f} \"\n",
        "                            f\"Train Loss: {train_loss/len(train_loader):.2f} \"\n",
        "                            f\"Val Acc: {val_acc/len(val_loader)* 100 :.2f}  \"\n",
        "                            f\"Val loss: {val_loss/len(val_loader):.2f} \")\n",
        "\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for x, y_start, y_length in test_loader:\n",
        "        loss, acc = eval_step(model, x, y_start, y_length)\n",
        "        test_loss += loss\n",
        "        test_acc += acc\n",
        "\n",
        "    print(f\"Test accuracy: {test_acc/len(test_loader)*100 :.2f}\")\n",
        "\n",
        "    pbar.close()\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "    return model"
      ],
      "id": "MEzuAKHrioFv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017478,
          "end_time": "2022-04-09T14:37:40.752397",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.734919",
          "status": "completed"
        },
        "tags": [],
        "id": "XhKuCD2XioFw"
      },
      "source": [
        "Finally, we can train the model.\n",
        "In this setup, we will use a single encoder block and a single head in the Multi-Head Attention.\n",
        "This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted\n",
        "as an \"explanation\" of the predictions (compared to the other papers above dealing with deep Transformers)."
      ],
      "id": "XhKuCD2XioFw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:40.788473Z",
          "iopub.status.busy": "2022-04-09T14:37:40.787980Z",
          "iopub.status.idle": "2022-04-09T14:37:44.274530Z",
          "shell.execute_reply": "2022-04-09T14:37:44.273926Z"
        },
        "papermill": {
          "duration": 3.505945,
          "end_time": "2022-04-09T14:37:44.275998",
          "exception": false,
          "start_time": "2022-04-09T14:37:40.770053",
          "status": "completed"
        },
        "tags": [],
        "id": "027cCDeCioFw"
      },
      "outputs": [],
      "source": [
        "detect0_model = TransformerPredictor(\n",
        "    input_dim=10,\n",
        "    model_dim=32,\n",
        "    num_heads=1,\n",
        "    num_start=12,\n",
        "    num_length=4,\n",
        "    num_layers=1,\n",
        "    dropout=0.0,\n",
        ")\n",
        "optimizer = optim.AdamW(detect0_model.parameters(), lr=0.001)\n",
        "\n",
        "detect0_model = train_model(detect0_model, train_dl, val_dl, test_dl, optimizer)"
      ],
      "id": "027cCDeCioFw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017964,
          "end_time": "2022-04-09T14:37:44.392392",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.374428",
          "status": "completed"
        },
        "tags": [],
        "id": "j4DXuS1BioFx"
      },
      "source": [
        "Can you check the model works?"
      ],
      "id": "j4DXuS1BioFx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-09T14:37:44.429196Z",
          "iopub.status.busy": "2022-04-09T14:37:44.428820Z",
          "iopub.status.idle": "2022-04-09T14:37:44.436248Z",
          "shell.execute_reply": "2022-04-09T14:37:44.435661Z"
        },
        "papermill": {
          "duration": 0.027384,
          "end_time": "2022-04-09T14:37:44.437643",
          "exception": false,
          "start_time": "2022-04-09T14:37:44.410259",
          "status": "completed"
        },
        "tags": [],
        "id": "F7IWHIubioFy"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "### YOUR CODE HERE! ###\n",
        "#######################"
      ],
      "id": "F7IWHIubioFy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "And that's it! Remember that you can use dedicated learnable tokens to accumulate features useful for a global task.\n",
        "\n",
        "You can try to complexify the task if you are interested! For instance you could try:\n",
        "*   Checking if a particular digit is in the list\n",
        "*   Finding out what the highest digit is\n",
        "*   Finding out what the most frequent digit is\n",
        "\n"
      ],
      "metadata": {
        "id": "CRtbUvQhMdYX"
      },
      "id": "CRtbUvQhMdYX"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "id,colab,colab_type,-all",
      "formats": "ipynb,py:percent",
      "main_language": "python"
    },
    "language_info": {
      "name": "python"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 72.589227,
      "end_time": "2022-04-09T14:38:41.893966",
      "environment_variables": {},
      "exception": null,
      "input_path": "course_UvA-DL/05-transformers-and-MH-attention/Transformers_MHAttention.ipynb",
      "output_path": ".notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb",
      "parameters": {},
      "start_time": "2022-04-09T14:37:29.304739",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}